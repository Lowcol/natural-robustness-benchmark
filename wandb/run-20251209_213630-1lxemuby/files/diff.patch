diff --git a/src/train_natural_rad.py b/src/train_natural_rad.py
index ad5feb3..5e7f5e8 100644
--- a/src/train_natural_rad.py
+++ b/src/train_natural_rad.py
@@ -14,8 +14,7 @@ from torch.distributions.categorical import Categorical
 from torch.utils.tensorboard import SummaryWriter
 import torch.nn.functional as F
 
-# NOTE: Ensure your wrapper file is named 'wrappers.py' or 'atari_wrappers.py' to match this import
-from wrappers import (  
+from atari_wrappers import (  
     ClipRewardEnv,
     EpisodicLifeEnv,
     FireResetEnv,
@@ -43,8 +42,10 @@ class Args:
     """the entity (team) of wandb's project"""
     capture_video: bool = True
     """whether to capture videos of the agent performances (check out `videos` folder)"""
-    natural_video_folder: str = "videos-noise" # Default folder for easier running
+    natural_video_folder: str = None
     """path to folder containing background videos for natural variant"""
+    log_aug_samples: bool = True
+    """if toggled, log a sample of raw vs augmented observations to TensorBoard"""
 
     # Algorithm specific arguments
     env_id: str = "BreakoutNoFrameskip-v4"
@@ -115,11 +116,7 @@ def rad_random_crop(imgs, size=84):
 
 def make_env(env_id, idx, capture_video, run_name, natural_video_folder=None):
     def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos-training-rad/{run_name}")
-        else:
-            env = gym.make(env_id)
+        env = gym.make(env_id, render_mode="rgb_array") if capture_video and idx == 0 else gym.make(env_id)
         env = gym.wrappers.RecordEpisodeStatistics(env)
         env = NoopResetEnv(env, noop_max=30)
         env = MaxAndSkipEnv(env, skip=4)
@@ -131,6 +128,14 @@ def make_env(env_id, idx, capture_video, run_name, natural_video_folder=None):
         # Add natural background wrapper BEFORE resizing and grayscaling
         if natural_video_folder is not None:
             env = NaturalBackgroundWrapper(env, natural_video_folder)
+
+        # Ensure recorded video plays at expected speed (60fps base / 4-frame skip = 15fps)
+        if capture_video and env.metadata.get("render_fps") in (None, 0):
+            env.metadata["render_fps"] = 15
+
+        # Record after augmentation so the saved video shows the natural background
+        if capture_video and idx == 0:
+            env = gym.wrappers.RecordVideo(env, f"videos-training-rad/{run_name}")
         
         env = gym.wrappers.ResizeObservation(env, (84, 84))
         env = gym.wrappers.GrayScaleObservation(env)
@@ -230,6 +235,7 @@ if __name__ == "__main__":
     next_obs, _ = envs.reset(seed=args.seed)
     next_obs = torch.Tensor(next_obs).to(device)
     next_done = torch.zeros(args.num_envs).to(device)
+    logged_aug_vis = False
 
     for iteration in range(1, args.num_iterations + 1):
         # Annealing the rate if instructed to do so.
@@ -248,6 +254,21 @@ if __name__ == "__main__":
                 # --- APPLY RAD (Random Crop) ---
                 # We crop the observation BEFORE the agent sees it
                 aug_obs = rad_random_crop(next_obs)
+                # Optionally log a sample of raw vs augmented for env 0 (once)
+                if args.log_aug_samples and not logged_aug_vis:
+                    raw_img = next_obs[0].detach().cpu()
+                    aug_img = aug_obs[0].detach().cpu()
+
+                    def to_chw(img):
+                        # If channels are last, move them to CHW for TensorBoard
+                        if img.ndim == 3 and img.shape[0] not in (1, 3, 4):
+                            img = img.permute(2, 0, 1)
+                        return img
+
+                    writer.add_image("debug/raw_obs_env0", to_chw(raw_img) / 255.0, global_step)
+                    writer.add_image("debug/aug_obs_env0", to_chw(aug_img) / 255.0, global_step)
+                    logged_aug_vis = True
+
                 action, logprob, _, value = agent.get_action_and_value(aug_obs)
                 # -------------------------------
                 values[step] = value.flatten()
diff --git a/wandb/run-20251206_141845-4ztow9j4/files/code/cleanrl/ppo_atari.py b/wandb/run-20251206_141845-4ztow9j4/files/code/cleanrl/ppo_atari.py
deleted file mode 100644
index 7b8cbcd..0000000
--- a/wandb/run-20251206_141845-4ztow9j4/files/code/cleanrl/ppo_atari.py
+++ /dev/null
@@ -1,344 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_ataripy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.categorical import Categorical
-from torch.utils.tensorboard import SummaryWriter
-
-from cleanrl_utils.atari_wrappers import (  # isort:skip
-    ClipRewardEnv,
-    EpisodicLifeEnv,
-    FireResetEnv,
-    MaxAndSkipEnv,
-    NoopResetEnv,
-)
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanrl_ppo_atari"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = True
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "BreakoutNoFrameskip-v4"
-    """the id of the environment"""
-    total_timesteps: int = 10000000
-    """total timesteps of the experiments"""
-    learning_rate: float = 2.5e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 8
-    """the number of parallel game environments"""
-    num_steps: int = 128
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 4
-    """the number of mini-batches"""
-    update_epochs: int = 4
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.1
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.01
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = NoopResetEnv(env, noop_max=30)
-        env = MaxAndSkipEnv(env, skip=4)
-        env = EpisodicLifeEnv(env)
-        if "FIRE" in env.unwrapped.get_action_meanings():
-            env = FireResetEnv(env)
-        env = ClipRewardEnv(env)
-        env = gym.wrappers.ResizeObservation(env, (84, 84))
-        env = gym.wrappers.GrayScaleObservation(env)
-        env = gym.wrappers.FrameStack(env, 4)
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.network = nn.Sequential(
-            layer_init(nn.Conv2d(4, 32, 8, stride=4)),
-            nn.ReLU(),
-            layer_init(nn.Conv2d(32, 64, 4, stride=2)),
-            nn.ReLU(),
-            layer_init(nn.Conv2d(64, 64, 3, stride=1)),
-            nn.ReLU(),
-            nn.Flatten(),
-            layer_init(nn.Linear(64 * 7 * 7, 512)),
-            nn.ReLU(),
-        )
-        self.actor = layer_init(nn.Linear(512, envs.single_action_space.n), std=0.01)
-        self.critic = layer_init(nn.Linear(512, 1), std=1)
-
-    def get_value(self, x):
-        return self.critic(self.network(x / 255.0))
-
-    def get_action_and_value(self, x, action=None):
-        hidden = self.network(x / 255.0)
-        logits = self.actor(hidden)
-        probs = Categorical(logits=logits)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = args.total_timesteps // args.batch_size
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name) for i in range(args.num_envs)],
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Discrete), "only discrete action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    # --- NEW: SAVE MODEL BLOCK ---
-    save_path = f"runs/{run_name}/{args.exp_name}.cleanrl_model"
-    # Ensure directory exists (usually done by SummaryWriter, but purely for safety)
-    os.makedirs(os.path.dirname(save_path), exist_ok=True)
-    
-    torch.save(agent.state_dict(), save_path)
-    print(f"Model saved to {save_path}")
-
-    if args.track:
-        import wandb
-        artifact = wandb.Artifact(f"model-{args.env_id}", type="model")
-        artifact.add_file(save_path)
-        wandb.log_artifact(artifact)
-    # -----------------------------
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/wandb/run-20251206_141845-4ztow9j4/files/conda-environment.yaml b/wandb/run-20251206_141845-4ztow9j4/files/conda-environment.yaml
deleted file mode 100644
index e69de29..0000000
diff --git a/wandb/run-20251206_141845-4ztow9j4/files/config.yaml b/wandb/run-20251206_141845-4ztow9j4/files/config.yaml
deleted file mode 100644
index 04ffabe..0000000
--- a/wandb/run-20251206_141845-4ztow9j4/files/config.yaml
+++ /dev/null
@@ -1,113 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/cleanrl/ppo_atari.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.0
-    start_time: 1765048725.052283
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.10.0
-      5: 0.13.11
-      8:
-      - 3
-      - 5
-anneal_lr:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 1024
-capture_video:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.1
-clip_vloss:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.01
-env_id:
-  desc: null
-  value: BreakoutNoFrameskip-v4
-exp_name:
-  desc: null
-  value: ppo_atari
-gae_lambda:
-  desc: null
-  value: 0.95
-gamma:
-  desc: null
-  value: 0.99
-learning_rate:
-  desc: null
-  value: 0.00025
-max_grad_norm:
-  desc: null
-  value: 0.5
-minibatch_size:
-  desc: null
-  value: 256
-norm_adv:
-  desc: null
-  value: true
-num_envs:
-  desc: null
-  value: 8
-num_iterations:
-  desc: null
-  value: 9765
-num_minibatches:
-  desc: null
-  value: 4
-num_steps:
-  desc: null
-  value: 128
-seed:
-  desc: null
-  value: 1
-target_kl:
-  desc: null
-  value: null
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 10000000
-track:
-  desc: null
-  value: true
-update_epochs:
-  desc: null
-  value: 4
-vf_coef:
-  desc: null
-  value: 0.5
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanrl_ppo_atari
diff --git a/wandb/run-20251206_141845-4ztow9j4/files/diff.patch b/wandb/run-20251206_141845-4ztow9j4/files/diff.patch
deleted file mode 100644
index b717a6e..0000000
--- a/wandb/run-20251206_141845-4ztow9j4/files/diff.patch
+++ /dev/null
@@ -1,109 +0,0 @@
-diff --git a/cleanrl/dqn_atari.py b/cleanrl/dqn_atari.py
-index 1561405..efffd7c 100644
---- a/cleanrl/dqn_atari.py
-+++ b/cleanrl/dqn_atari.py
-@@ -39,7 +39,7 @@ class Args:
-     """the wandb's project name"""
-     wandb_entity: str = None
-     """the entity (team) of wandb's project"""
--    capture_video: bool = False
-+    capture_video: bool = True
-     """whether to capture videos of the agent performances (check out `videos` folder)"""
-     save_model: bool = False
-     """whether to save model into the `runs/{run_name}` folder"""
-@@ -51,9 +51,9 @@ class Args:
-     # Algorithm specific arguments
-     env_id: str = "BreakoutNoFrameskip-v4"
-     """the id of the environment"""
--    total_timesteps: int = 800000
-+    total_timesteps: int = 10000000
-     """total timesteps of the experiments"""
--    learning_rate: float = 0.00025
-+    learning_rate: float = 0.0001
-     """the learning rate of the optimizer"""
-     num_envs: int = 1
-     """the number of parallel game environments"""
-@@ -63,7 +63,7 @@ class Args:
-     """the discount factor gamma"""
-     tau: float = 1.0
-     """the target network update rate"""
--    target_network_frequency: int = 10000
-+    target_network_frequency: int = 1000
-     """the timesteps it takes to update the target network"""
-     batch_size: int = 32
-     """the batch size of sample from the reply memory"""
-@@ -73,7 +73,7 @@ class Args:
-     """the ending epsilon for exploration"""
-     exploration_fraction: float = 0.10
-     """the fraction of `total-timesteps` it takes from start-e to go end-e"""
--    learning_starts: int = 100000
-+    learning_starts: int = 80000
-     """timestep to start learning"""
-     train_frequency: int = 4
-     """the frequency of training"""
-diff --git a/cleanrl/ppo_atari.py b/cleanrl/ppo_atari.py
-index f1b21d2..7b8cbcd 100644
---- a/cleanrl/ppo_atari.py
-+++ b/cleanrl/ppo_atari.py
-@@ -32,13 +32,13 @@ class Args:
-     """if toggled, `torch.backends.cudnn.deterministic=False`"""
-     cuda: bool = True
-     """if toggled, cuda will be enabled by default"""
--    track: bool = False
-+    track: bool = True
-     """if toggled, this experiment will be tracked with Weights and Biases"""
--    wandb_project_name: str = "cleanRL"
-+    wandb_project_name: str = "cleanrl_ppo_atari"
-     """the wandb's project name"""
-     wandb_entity: str = None
-     """the entity (team) of wandb's project"""
--    capture_video: bool = False
-+    capture_video: bool = True
-     """whether to capture videos of the agent performances (check out `videos` folder)"""
- 
-     # Algorithm specific arguments
-@@ -325,5 +325,20 @@ if __name__ == "__main__":
-         print("SPS:", int(global_step / (time.time() - start_time)))
-         writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
- 
-+    # --- NEW: SAVE MODEL BLOCK ---
-+    save_path = f"runs/{run_name}/{args.exp_name}.cleanrl_model"
-+    # Ensure directory exists (usually done by SummaryWriter, but purely for safety)
-+    os.makedirs(os.path.dirname(save_path), exist_ok=True)
-+    
-+    torch.save(agent.state_dict(), save_path)
-+    print(f"Model saved to {save_path}")
-+
-+    if args.track:
-+        import wandb
-+        artifact = wandb.Artifact(f"model-{args.env_id}", type="model")
-+        artifact.add_file(save_path)
-+        wandb.log_artifact(artifact)
-+    # -----------------------------
-+
-     envs.close()
--    writer.close()
-+    writer.close()
-\ No newline at end of file
-diff --git a/cleanrl/rainbow_atari.py b/cleanrl/rainbow_atari.py
-index 71e5a15..4a8887b 100644
---- a/cleanrl/rainbow_atari.py
-+++ b/cleanrl/rainbow_atari.py
-@@ -41,7 +41,7 @@ class Args:
-     """the wandb's project name"""
-     wandb_entity: str = None
-     """the entity (team) of wandb's project"""
--    capture_video: bool = False
-+    capture_video: bool = True
-     """whether to capture videos of the agent performances (check out `videos` folder)"""
-     save_model: bool = False
-     """whether to save model into the `runs/{run_name}` folder"""
-@@ -58,7 +58,7 @@ class Args:
-     """the learning rate of the optimizer"""
-     num_envs: int = 1
-     """the number of parallel game environments"""
--    buffer_size: int = 1000000
-+    buffer_size: int = 500000
-     """the replay memory buffer size"""
-     gamma: float = 0.99
-     """the discount factor gamma"""
diff --git a/wandb/run-20251206_141845-4ztow9j4/files/media/videos/videos_0_63796348ff3963c3eca6.mp4 b/wandb/run-20251206_141845-4ztow9j4/files/media/videos/videos_0_63796348ff3963c3eca6.mp4
deleted file mode 100644
index 89a4ad9..0000000
Binary files a/wandb/run-20251206_141845-4ztow9j4/files/media/videos/videos_0_63796348ff3963c3eca6.mp4 and /dev/null differ
diff --git a/wandb/run-20251206_141845-4ztow9j4/files/media/videos/videos_9_8e67b6968acd926dc3ec.mp4 b/wandb/run-20251206_141845-4ztow9j4/files/media/videos/videos_9_8e67b6968acd926dc3ec.mp4
deleted file mode 100644
index e490574..0000000
Binary files a/wandb/run-20251206_141845-4ztow9j4/files/media/videos/videos_9_8e67b6968acd926dc3ec.mp4 and /dev/null differ
diff --git a/wandb/run-20251206_141845-4ztow9j4/files/requirements.txt b/wandb/run-20251206_141845-4ztow9j4/files/requirements.txt
deleted file mode 100644
index 763f5e3..0000000
--- a/wandb/run-20251206_141845-4ztow9j4/files/requirements.txt
+++ /dev/null
@@ -1,86 +0,0 @@
-absl-py==1.4.0
-ale-py==0.8.1
-appdirs==1.4.4
-autorom-accept-rom-license==0.6.1
-autorom==0.4.2
-cachetools==5.3.0
-certifi==2023.5.7
-cfgv==3.3.1
-charset-normalizer==3.1.0
-cleanrl==2.0.0b1
-click==8.1.3
-cloudpickle==2.2.1
-colorama==0.4.4
-commonmark==0.9.1
-decorator==4.4.2
-distlib==0.3.6
-docker-pycreds==0.4.0
-docstring-parser==0.15
-farama-notifications==0.0.4
-filelock==3.12.0
-fsspec==2025.5.1
-gitdb==4.0.10
-gitpython==3.1.31
-google-auth-oauthlib==0.4.6
-google-auth==2.18.0
-grpcio==1.54.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.29.1
-huggingface-hub==0.11.1
-identify==2.5.24
-idna==3.4
-imageio-ffmpeg==0.3.0
-imageio==2.28.1
-importlib-resources==5.12.0
-jinja2==3.1.2
-markdown==3.3.7
-markupsafe==2.1.2
-moviepy==1.0.3
-mpmath==1.3.0
-networkx==3.4.2
-nodeenv==1.7.0
-numpy==1.24.4
-oauthlib==3.2.2
-opencv-python==4.7.0.72
-packaging==23.1
-pathtools==0.1.2
-pillow==9.5.0
-pip==25.3
-platformdirs==3.5.0
-pre-commit==2.21.0
-proglog==0.1.10
-protobuf==3.20.3
-psutil==5.9.5
-pyasn1-modules==0.3.0
-pyasn1==0.5.0
-pygame==2.1.0
-pygments==2.15.1
-pyyaml==6.0.1
-requests-oauthlib==1.3.1
-requests==2.30.0
-rich==11.2.0
-rsa==4.7.2
-sentry-sdk==1.22.2
-setproctitle==1.3.2
-setuptools==67.7.2
-shimmy==1.1.0
-shtab==1.6.4
-six==1.16.0
-smmap==5.0.0
-sympy==1.14.0
-tenacity==8.2.3
-tensorboard-data-server==0.6.1
-tensorboard-plugin-wit==1.8.1
-tensorboard==2.11.2
-torch==2.4.1+cu121
-torchaudio==2.4.1+cu121
-torchvision==0.19.1+cu121
-tqdm==4.65.0
-typing-extensions==4.14.1
-tyro==0.5.10
-urllib3==1.26.15
-virtualenv==20.21.0
-wandb==0.13.11
-werkzeug==2.2.3
-wheel==0.40.0
\ No newline at end of file
diff --git a/wandb/run-20251206_141845-4ztow9j4/files/wandb-metadata.json b/wandb/run-20251206_141845-4ztow9j4/files/wandb-metadata.json
deleted file mode 100644
index 7f6c816..0000000
--- a/wandb/run-20251206_141845-4ztow9j4/files/wandb-metadata.json
+++ /dev/null
@@ -1,50 +0,0 @@
-{
-    "os": "Windows-10-10.0.19045-SP0",
-    "python": "3.10.0",
-    "heartbeatAt": "2025-12-06T19:18:45.844595",
-    "startedAt": "2025-12-06T19:18:45.032126",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "C:\\Users\\godin\\OneDrive\\Bureau\\cleanrl\\cleanrl\\ppo_atari.py",
-    "codePath": "cleanrl\\ppo_atari.py",
-    "git": {
-        "remote": "https://github.com/Lowcol/cleanrl.git",
-        "commit": "a49418e9cd68e9d8a4159b13801b6e2d4313100b"
-    },
-    "email": "antoine.godin.1@ens.etsmtl.ca",
-    "root": "C:/Users/godin/OneDrive/Bureau/cleanrl",
-    "host": "DESKTOP-4BFTBQC",
-    "username": "godin",
-    "executable": "C:\\Users\\godin\\anaconda3\\envs\\cleanRL\\python.exe",
-    "cpu_count": 8,
-    "cpu_count_logical": 8,
-    "cpu_freq": {
-        "current": 3600.0,
-        "min": 0.0,
-        "max": 3600.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3600.0,
-            "min": 0.0,
-            "max": 3600.0
-        }
-    ],
-    "disk": {
-        "total": 952.6542930603027,
-        "used": 401.21782302856445
-    },
-    "gpu": "NVIDIA GeForce RTX 2060",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 2060",
-            "memory_total": 6442450944
-        }
-    ],
-    "memory": {
-        "total": 15.920894622802734
-    }
-}
diff --git a/wandb/run-20251206_141845-4ztow9j4/files/wandb-summary.json b/wandb/run-20251206_141845-4ztow9j4/files/wandb-summary.json
deleted file mode 100644
index 3e1fec2..0000000
--- a/wandb/run-20251206_141845-4ztow9j4/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"videos": {"_type": "video-file", "sha256": "8e67b6968acd926dc3ec929aa0e77d5066a3d893395e180af7205278672ba055", "size": 469924, "path": "media/videos/videos_9_8e67b6968acd926dc3ec.mp4"}, "_timestamp": 1765056989.8521814, "_runtime": 8264.799898386002, "_step": 9, "_wandb": {"runtime": 14504}}
\ No newline at end of file
diff --git a/wandb/run-20251206_141845-4ztow9j4/run-4ztow9j4.wandb b/wandb/run-20251206_141845-4ztow9j4/run-4ztow9j4.wandb
deleted file mode 100644
index 067e5f6..0000000
Binary files a/wandb/run-20251206_141845-4ztow9j4/run-4ztow9j4.wandb and /dev/null differ
