diff --git a/cleanrl/dqn_atari.py b/cleanrl/dqn_atari.py
index 1561405..efffd7c 100644
--- a/cleanrl/dqn_atari.py
+++ b/cleanrl/dqn_atari.py
@@ -39,7 +39,7 @@ class Args:
     """the wandb's project name"""
     wandb_entity: str = None
     """the entity (team) of wandb's project"""
-    capture_video: bool = False
+    capture_video: bool = True
     """whether to capture videos of the agent performances (check out `videos` folder)"""
     save_model: bool = False
     """whether to save model into the `runs/{run_name}` folder"""
@@ -51,9 +51,9 @@ class Args:
     # Algorithm specific arguments
     env_id: str = "BreakoutNoFrameskip-v4"
     """the id of the environment"""
-    total_timesteps: int = 800000
+    total_timesteps: int = 10000000
     """total timesteps of the experiments"""
-    learning_rate: float = 0.00025
+    learning_rate: float = 0.0001
     """the learning rate of the optimizer"""
     num_envs: int = 1
     """the number of parallel game environments"""
@@ -63,7 +63,7 @@ class Args:
     """the discount factor gamma"""
     tau: float = 1.0
     """the target network update rate"""
-    target_network_frequency: int = 10000
+    target_network_frequency: int = 1000
     """the timesteps it takes to update the target network"""
     batch_size: int = 32
     """the batch size of sample from the reply memory"""
@@ -73,7 +73,7 @@ class Args:
     """the ending epsilon for exploration"""
     exploration_fraction: float = 0.10
     """the fraction of `total-timesteps` it takes from start-e to go end-e"""
-    learning_starts: int = 100000
+    learning_starts: int = 80000
     """timestep to start learning"""
     train_frequency: int = 4
     """the frequency of training"""
diff --git a/cleanrl/ppo_atari.py b/cleanrl/ppo_atari.py
index f1b21d2..7b8cbcd 100644
--- a/cleanrl/ppo_atari.py
+++ b/cleanrl/ppo_atari.py
@@ -32,13 +32,13 @@ class Args:
     """if toggled, `torch.backends.cudnn.deterministic=False`"""
     cuda: bool = True
     """if toggled, cuda will be enabled by default"""
-    track: bool = False
+    track: bool = True
     """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
+    wandb_project_name: str = "cleanrl_ppo_atari"
     """the wandb's project name"""
     wandb_entity: str = None
     """the entity (team) of wandb's project"""
-    capture_video: bool = False
+    capture_video: bool = True
     """whether to capture videos of the agent performances (check out `videos` folder)"""
 
     # Algorithm specific arguments
@@ -325,5 +325,20 @@ if __name__ == "__main__":
         print("SPS:", int(global_step / (time.time() - start_time)))
         writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
 
+    # --- NEW: SAVE MODEL BLOCK ---
+    save_path = f"runs/{run_name}/{args.exp_name}.cleanrl_model"
+    # Ensure directory exists (usually done by SummaryWriter, but purely for safety)
+    os.makedirs(os.path.dirname(save_path), exist_ok=True)
+    
+    torch.save(agent.state_dict(), save_path)
+    print(f"Model saved to {save_path}")
+
+    if args.track:
+        import wandb
+        artifact = wandb.Artifact(f"model-{args.env_id}", type="model")
+        artifact.add_file(save_path)
+        wandb.log_artifact(artifact)
+    # -----------------------------
+
     envs.close()
-    writer.close()
+    writer.close()
\ No newline at end of file
diff --git a/cleanrl/rainbow_atari.py b/cleanrl/rainbow_atari.py
index 71e5a15..4a8887b 100644
--- a/cleanrl/rainbow_atari.py
+++ b/cleanrl/rainbow_atari.py
@@ -41,7 +41,7 @@ class Args:
     """the wandb's project name"""
     wandb_entity: str = None
     """the entity (team) of wandb's project"""
-    capture_video: bool = False
+    capture_video: bool = True
     """whether to capture videos of the agent performances (check out `videos` folder)"""
     save_model: bool = False
     """whether to save model into the `runs/{run_name}` folder"""
@@ -58,7 +58,7 @@ class Args:
     """the learning rate of the optimizer"""
     num_envs: int = 1
     """the number of parallel game environments"""
-    buffer_size: int = 1000000
+    buffer_size: int = 500000
     """the replay memory buffer size"""
     gamma: float = 0.99
     """the discount factor gamma"""
